import streamlit as st
import pdfplumber
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
from io import BytesIO
from PIL import Image
import matplotlib.pyplot as plt

# -------------------------------
# Streamlit Page Config
# -------------------------------
st.set_page_config(
    page_title="Agentic AI - Smart Document Classifier",
    page_icon="ü§ñ",
    layout="wide"
)

# -------------------------------
# Custom Styling
# -------------------------------
st.markdown("""
    <style>
        .stApp {
            background-color: #F9FAFB;
        }
        h1 {
            text-align: center;
            color: #2F80ED;
        }
        .desc {
            text-align: center;
            color: #6C757D;
            font-size: 16px;
            margin-bottom: 30px;
        }
        .result-box {
            background-color: #E3F2FD;
            padding: 15px;
            border-radius: 10px;
            text-align: center;
            font-size: 18px;
        }
        .footer {
            text-align: center;
            color: gray;
            font-size: 13px;
            margin-top: 40px;
        }
    </style>
""", unsafe_allow_html=True)

# -------------------------------
# Title Section
# -------------------------------
st.markdown("<h1>ü§ñ Agentic AI - Smart Document Classifier</h1>", unsafe_allow_html=True)
st.markdown("<p class='desc'>Upload any insurance document ‚Äî Invoice, Claim Form, or Inspection Report ‚Äî and let AI classify it automatically.</p>", unsafe_allow_html=True)

# -------------------------------
# Load Embedding Model
# -------------------------------
@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# -------------------------------
# Predefined Categories
# -------------------------------
categories = {
    "Invoice": [
        "invoice", "bill", "amount due", "subtotal", "tax", "payment terms", "billed to"
    ],
    "Claim Form": [
        "insurance claim", "policy number", "incident details", "claim type",
        "insured name", "loss description", "signature of claimant"
    ],
    "Inspection Report": [
        "inspection report", "vehicle assessment", "inspection date",
        "damage description", "inspector name", "findings", "recommendations"
    ],
}

# -------------------------------
# Extract Text Function
# -------------------------------
def extract_text_from_pdf(file):
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text.strip()

# -------------------------------
# Improved Classification
# -------------------------------
def classify_document(text):
    chunks = [text[i:i+500] for i in range(0, len(text), 500)]
    chunk_embeddings = model.encode(chunks, convert_to_numpy=True)

    category_scores = {}
    for cat, keywords in categories.items():
        cat_embedding = model.encode([" ".join(keywords)], convert_to_numpy=True)
        similarities = np.dot(chunk_embeddings, cat_embedding.T) / (
            np.linalg.norm(chunk_embeddings, axis=1, keepdims=True) * np.linalg.norm(cat_embedding)
        )
        category_scores[cat] = float(np.mean(similarities))

    best_label = max(category_scores, key=category_scores.get)
    confidence = round(category_scores[best_label], 3)
    return best_label, confidence, category_scores

# -------------------------------
# Extract Important Fields
# -------------------------------
def extract_fields(text):
    fields = {}
    date_match = re.search(r"\b\d{2,4}[/-]\d{2}[/-]\d{2,4}\b", text)
    amount_match = re.search(r"[\$‚Çπ]\s?\d+[,.]?\d*", text)
    name_match = re.search(r"(?i)(?:Name|Customer|Client|Insured)[:\-]?\s*([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)", text)

    fields["Date"] = date_match.group(0) if date_match else "Not found"
    fields["Amount"] = amount_match.group(0) if amount_match else "Not found"
    fields["Name"] = name_match.group(1) if name_match else "Not found"
    return fields

# -------------------------------
# File Upload
# -------------------------------
uploaded_file = st.file_uploader("üìÅ Upload a PDF document", type=["pdf"])

if uploaded_file:
    with st.spinner("üîç Analyzing your document... Please wait"):
        text = extract_text_from_pdf(uploaded_file)
        label, confidence, scores = classify_document(text)
        fields = extract_fields(text)

    st.success("‚úÖ Document analyzed successfully!")

    # -------------------------------
    # Show Results
    # -------------------------------
    st.markdown(f"<div class='result-box'><b>Predicted Document Type:</b> {label} <br> <b>Confidence:</b> {confidence}</div>", unsafe_allow_html=True)

    # -------------------------------
    # Show Similarity Chart
    # -------------------------------
    st.subheader("üìà Category Confidence Overview")
    fig, ax = plt.subplots(figsize=(6, 3))
    ax.barh(list(scores.keys()), list(scores.values()))
    ax.set_xlabel("Similarity Score")
    ax.set_title("Category-wise Confidence")
    st.pyplot(fig)

    # -------------------------------
    # Show Extracted Fields
    # -------------------------------
    st.subheader("üìã Key Extracted Fields")
    col1, col2, col3 = st.columns(3)
    col1.metric("üìÖ Date", fields["Date"])
    col2.metric("üí∞ Amount", fields["Amount"])
    col3.metric("üßç Name", fields["Name"])

    # -------------------------------
    # View Extracted Text
    # -------------------------------
    with st.expander("üìú View Extracted Text Preview"):
        st.text_area("Extracted Text", text[:2000] + "...", height=250)

else:
    st.info("‚¨ÜÔ∏è Please upload a PDF to begin analysis.")

# -------------------------------
# Footer
# -------------------------------
st.markdown("""
<hr>
<p class='footer'>
Built using Streamlit & SentenceTransformers | Agentic AI ¬© 2025
</p>
""", unsafe_allow_html=True)
